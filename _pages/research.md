---
layout: page
permalink: /research/
title: work in progress
description: 
nav: true
nav_order: 2
---

[Research Statement](phkieval.com/assets/pdf/Research_Statement.pdf)
s
Representation Learning without Representationalism (forthcoming)

<p>
  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse1" aria-expanded="false" aria-controls="collapse1">
 Abstract
  </button>
</p>

<div class="collapse" id="collapse1">
  <div class="card card-body">
   Both machine learning research and philosophy of model-based science appeal to representations. Given this shared emphasis, it seems natural to conclude that we should understand the epistemic worth of deep learning models in science in terms of their capacity to represent the world. I argue that this assumption is a mistake. I distinguish the prevailing notion of representation used in machine learning research from the concept of scientific representation that figures in the philosophy of model-based science. I argue that the former cannot do the work of the latter in model-based inferences. I then defend a deflationary, artifactualist approach to machine learning models in science, which aims to understand their epistemic worth in terms of the material practices of constructing and applying them. Machine learning models are instruments that we use to facilitate our epistemic activities in science.    
  </div>
</div>

<p>


The Measure of a Manifold (under review)  

 <p> <button class="btn " type="button" data-toggle="collapse" data-target="#collapse3" aria-expanded="false" aria-controls="collapse3">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse3">
  <div class="card card-body">
    The neural manifold hypothesis states that the activity of a neural population encodes information along a low-dimensional structure embedded in high-dimensional neural activity. Investigators purport to uncover these structures by using data-analytic techniques to reduce the dimensionality of patters of neural activity. But, the concept of a neural manifold remains contested, leaving manifolds' causal and explanatory significance ambiguous. In this paper, I will describe how these ongoing and contested attempts to characterize neural manifolds inform what standards will be used to evaluate explanations in neuroscientific practice. My analysis shows that realist characterizations of neural manifolds constrain the extent to which localization is possible. The neural manifold hypothesis thus gives rise to a conception of neural computation realized by continuous dynamics along the surface of a manifold. The computational explanations are non-mechanistic and determining their causal-explanatory status is a matter of ongoing research that requires coordination with interventions. Finally, in light of their contested nature, I defend a methodological approach to inverstigating neural manifolds that I call operational agnosticism. Neuroscientists should embrace operationalism as a useful tool for clarifying methodological assumptions and making progress amidst uncertainty on the grounds that operationalism is agnostic between competing explanatory frameworks.
  </div>
</div>
<p>

Neural Scaling Laws and Science Without Theory (under review)

<p>  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse4" aria-expanded="false" aria-controls="collapse4">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse4">
  <div class="card card-body">
    The purpose of this paper is to scrutinize the relationship between neural scaling laws and what Andrews (2025) has recently called the "theory-free ideal" in science. I argue that, in science, scaling up the size of a neural network model cannot replace principled model design. The assumption that scaling alone can achieve accurate results rests on a naive undertanding of data as rawm objective windows into reality. Instead, training complex neural network models requires significant pre-processing and conceptual work, challenging the idea that these data-driven methods can be theory-free. Moreover, scaling up depends on the availability of vast quantities of high-quality data at valumes far beyond what is currently possible. This scaling bottleneck necessitates informed design choices about model architectures. I illustrate this fact by examining how current successes in scientific deep learning already depend on extensive data augmentation practices. Drawing on Sabina Leonelli's (2016) relational conception of data, I argue that this case shows that data packaging strategies required to train sophisticated deep learning models depend on a significant degree of judgmment to make available data ready for consumption by the model. The judgment involvd at this stage further dispels any notion of "raw" data as an epistemically privileged given, an idea that in turn feeds into popular narrtives concerning data-driven science's alleged theory-free turn.
  </div>
</div>
<p>


[Deep Learning as Method-Learning](https://philsci-archive.pitt.edu/23489/) (w/ Oscar Westeblad) (in preparation)

<p>  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse5" aria-expanded="false" aria-controls="collapse5">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse5">
  <div class="card card-body">
We claim that scientists working with deep learning (DL) models exhibit a form of pragmatic understanding that is not reducible to or dependent on explanation. This pragmatic understanding comprises a set of learned methodological principles that underlie DL model design-choices and secure their reliability. We illustrate this action-oriented pragmatic understanding with a case study
of AlphaFold2, highlighting the interplay between background knowledge of a problem and methodological choices involving techniques for constraining how a model learns from data. Building successful models requires pragmatic understanding to apply modelling strategies that encourage the model to learn data patterns that will facilitate reliable generalisation.
  </div>
</div>
<p>


Artificial Empathy, Social Affordances, and Moral Development (w/ Cameron Buckner) (in preparation)

<p>  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse6" aria-expanded="false" aria-controls="collapse6">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse6">
  <div class="card card-body">
Private firms are flooding the market with conversational AI agents based on large language models (LLMs). Efforts are already underway to train these chatbots to be more socially satisfying by endowing them with more empatheic responses to user prompts. In this paper, we argue that despite the apparently impressibe improvement in emotional identification by recent chatbots, the prevailing approaches to artifical empathy in LLMs suffer from a spectatorial conception of social cognition which is unlikely to lead to thicker and extended interactions that promote user well-being. Our perspective aims to redefine the agenda for research on artificial empathy in AI agents by emphasize the role of affective cognitions as affording opportunities for mutual social calibration and moral development. 
  </div>
</div>
<p>


Formal Anlogies and Model Transfer in Scientific Deep Learning (in preparation)

<p>  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse7" aria-expanded="false" aria-controls="collapse7">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse7">
  <div class="card card-body">
In this paper, I examine the trans-disciplinary successes of deep learning, and I argue that attention to how deep learning architectures travel between fields reveals a distinctive form of interdisciplinary model transfer. Model transfer refers to the practice of applying a model that originates in one scientific domain to do novel epistemic work in a different domain.  Many historical cases of such cross-pollination have brought philosophical interest to the role of ``templates'' in model-building. A model template refers to a mathematical structure or computational method whose accompanying conceptual resources can be used to illuminate observed patterns in different kinds of systems \citep{Knuuttila2016}. Analogies between patterns observed in different kinds of systems warrant adopting the same model templates in different scientific domains. I argue that when it comes to deep learning, purely formal analogies mediate such model transfers. These analogies hold between qualitative patterns that modellers expect to find in the structure of very different kinds of data given their background knowledge of a scientific problem. I argue that the formal analogies that motivate deep learning model transfers target structural features of the data themselves. Understanding how these formal analogies contrast with existing philosophical accounts of analogies in science thus requires examining how modellers work with conceptualisations of generic patterns or signatures associated with specific machine-readable data structures. 
  </div>
</div>
<p>


Modeling Cognition with Goal-Directed Neural Networks (in preparation)

<p>  <button class="btn " type="button" data-toggle="collapse" data-target="#collapse8" aria-expanded="false" aria-controls="collapse8">
 Abstract
  </button>
</p>
<div class="collapse" id="collapse8">
  <div class="card card-body">
A popular approach to studying cognition involves using artificial neural networks as surrogate models of the mechanisms that drive flexible and adaptive behaviour, such as learning, memory, perception, and language use. Mounting empirical evidence from computational cognitive neuroscience suggests that deep neural networks (DNNs) are useful tools for learning about regions of interest in the brain. For example, studies reveal that deep convolutional neural networks (or ConvNets) trained to solve object recognition tasks acquire internal representations that closely resemble processes in the primate ventral visual stream. Evidence for this correspondence came as a surprise because these models were not designed based on explicit neurobiological constraints. Instead, they are "goal-directed" models optimised to solve engineering tasks like image classification. Many now claim that goal-directed DNNs are the best models human cognitive faculties like visual object recognition and language processing. Recent critics claim that goal-directed DNNs fail to adequately explain cognitive processes. Such critiques tend to emphasise that (i) DNN models fail to replicate well-documented empirical findings reported in psychology and (ii) important aspects of DNNs lack biological plausibility and are thus not sufficiently similar to their target neural mechanisms. In this paper, I argue that these criticisms are misplaced, stemming from a lack of clarity about the explanatory targets of neural network models and an overly simplistic conception of model-based explanation. I examine a method of assessing the correspondence between goal-directed models and neural processing called representational similarity analysis (RSA). I argue that RSA functions in practice as a tool for identifying the higher-order organisations of distributed representational states in both neural network models and their target systems. In doing so, neuroscientists use it as a defeasible guide to identifying generic information processing mechanisms that can be instantiated in different kinds of systems. By showing that DNNs and cortical circuits in the brain process information using similarly organised representations, neuroscientists gain some assurance that goal-direct DNNs can be used to draw surrogative inferences about the mechanisms underlying information processing in regions of interest in the brain. 
</div>
</div>
<p>

Symmetries as Non-Causal Constraints for Learning Cuasal Models (in draft)
<p>


How neural dynamics compute (in draft)